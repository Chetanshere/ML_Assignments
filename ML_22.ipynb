{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b00dfd",
   "metadata": {},
   "source": [
    "## 1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840685b8",
   "metadata": {},
   "source": [
    "you can try combining them into a voting ensemble, which will often give you even better results. It works better if the models are very different (e.g., an SVM classifier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is even better if they are trained on different training instances (that’s the whole point of bagging and pasting ensembles), but if not it will still work as long as the models are very different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c6dda",
   "metadata": {},
   "source": [
    "## 2. What&#39;s the difference between hard voting classifiers and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cec0f8",
   "metadata": {},
   "source": [
    "A hard voting classifier just counts the votes of each classifier in the ensemble and picks the class that gets the most votes. A soft voting classifier computes the average estimated class probability for each class and picks the class with the highest probability. This gives high-confidence votes more weight and often performs better, but it works only if every classifier is able to estimate class probabilities (e.g., for the SVM classifiers in Scikit-Learn you must set probability=True)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0a5e9",
   "metadata": {},
   "source": [
    "## 3. Is it possible to distribute a bagging ensemble&#39;s training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8273f",
   "metadata": {},
   "source": [
    "* Bagging - Yes it is possible to train distrubutedly in several serves since each predictors in the ensembles are independed of the others.\n",
    "\n",
    "* Same goes with pasting emsembles and random forests\n",
    "\n",
    "* Boosting - each predictors in a boosting emsemble is build based on the previous predictor, so training is sequential. It not possible to train them in different serves.\n",
    "\n",
    "* Stacking - All the predictors in the given layers are independent of each other, so they can be trained in parallel on multiple serves. But training has to be done only once the previous layer is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21befdbd",
   "metadata": {},
   "source": [
    "## 4. What is the advantage of evaluating out of the bag?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2563483",
   "metadata": {},
   "source": [
    "Out of the bag instances are those who are not picked for training. These are used for evaluating the model with out the need for an additional validation set. Witht this it is pissible to have a unbiased evaluation of the emsembles. With Out of bag instances we will have more instances aviable for training and our emseble can perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01eb57",
   "metadata": {},
   "source": [
    "## 5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ccb1c",
   "metadata": {},
   "source": [
    "When you are growing a tree in a Random Forest, only a random subset of the features is considered for splitting at each node. This is true as well for Extra- Trees, but they go one step further: rather than searching for the best possible thresholds, like regular Decision Trees do, they use random thresholds for each feature. This extra randomness acts like a form of regularization: if a Random Forest overfits the training data, Extra-Trees might perform better. Moreover, since Extra-Trees don’t search for the best possible thresholds, they are much faster to train than Random Forests. However, they are neither faster nor slower than Random Forests when making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08649438",
   "metadata": {},
   "source": [
    "## 6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6812e",
   "metadata": {},
   "source": [
    "To fix AdaBoost ensemble underfits the training data,\n",
    "\n",
    "1. increasing the number of estimators\n",
    "\n",
    "2. reducing the regularization hyperparameters of the base estimator\n",
    "\n",
    "3. try slightly increasing the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47459170",
   "metadata": {},
   "source": [
    "## 7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c987c5",
   "metadata": {},
   "source": [
    "To fix if Gradient Boosting ensemble overfits the training set,\n",
    "1. decreasing the learning rate.\n",
    "\n",
    "2. use early stopping to find the right number of predictors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
